{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c097f98a",
   "metadata": {},
   "source": [
    "\n",
    "# Chainlit + Ollama Chatbot Application (Jupyter Notebook Version)\n",
    "\n",
    "## 1️⃣ Importing Required Libraries\n",
    "\n",
    "```python\n",
    "from operator import itemgetter\n",
    "import os\n",
    "import ollama\n",
    "import subprocess\n",
    "import threading\n",
    "import requests\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import chainlit as cl\n",
    "from chainlit.data.sql_alchemy import SQLAlchemyDataLayer\n",
    "from chainlit.types import ThreadDict\n",
    "```\n",
    "\n",
    "## 2️⃣ Load Environment Variables\n",
    "\n",
    "```python\n",
    "load_dotenv()\n",
    "\n",
    "# Setting environment variables directly (hardcoded for demonstration)\n",
    "os.environ['CHAINLIT_AUTH_SECRET'] = \"r>>aPxK9Iwl%KMjr,sjeIoP@I.kGOLb*kwriPYwtW$S9vJVR2HYFh.JUc_0J:PF.\"\n",
    "os.environ['DATABASE_URL'] = \"postgresql+asyncpg://chainlit_user:securepassword@localhost:5532/chainlit_db\"\n",
    "```\n",
    "\n",
    "## 3️⃣ Ollama Server Initialization\n",
    "\n",
    "### Helper function to start Ollama Server\n",
    "```python\n",
    "def _ollama():\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "    subprocess.Popen(['ollama', 'serve'])\n",
    "```\n",
    "\n",
    "### Start Ollama in a separate thread\n",
    "```python\n",
    "def start_ollama():\n",
    "    thread = threading.Thread(target=_ollama)\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "```\n",
    "\n",
    "## 4️⃣ Authentication Callback\n",
    "\n",
    "### Password Authentication Function for Chainlit\n",
    "```python\n",
    "@cl.password_auth_callback\n",
    "def auth_callback(username: str, password: str):\n",
    "    return cl.User(identifier=username)\n",
    "```\n",
    "\n",
    "## 5️⃣ Chat Session Initialization\n",
    "\n",
    "### Initialize Chat Session and Start Ollama\n",
    "```python\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    start_ollama()\n",
    "    cl.user_session.set('chat_history', [])\n",
    "```\n",
    "\n",
    "## 6️⃣ Database Layer for Chat History Persistence\n",
    "\n",
    "```python\n",
    "@cl.data_layer\n",
    "def get_data_layer():\n",
    "    return SQLAlchemyDataLayer(conninfo=os.getenv(\"DATABASE_URL\"))\n",
    "```\n",
    "\n",
    "## 7️⃣ Resume Chat Session\n",
    "\n",
    "### Load chat history from previous sessions\n",
    "```python\n",
    "@cl.on_chat_resume\n",
    "async def on_chat_resume(thread: ThreadDict):\n",
    "    start_ollama()\n",
    "    cl.user_session.set(\"chat_history\", [])\n",
    "    \n",
    "    for message in thread['steps']:\n",
    "        if message['type'] == 'user_message':\n",
    "            cl.user_session.get(\"chat_history\").append(\n",
    "                {'role':'user', 'content': message['output']}\n",
    "            )\n",
    "        elif message['type'] == 'assistant_message':\n",
    "            cl.user_session.get(\"chat_history\").append(\n",
    "                {'role':'assistant', 'content': message['output']}\n",
    "            )\n",
    "```\n",
    "\n",
    "## 8️⃣ Chat Message Handling Logic\n",
    "\n",
    "### Main chat logic with streaming responses from Ollama\n",
    "```python\n",
    "@cl.on_message\n",
    "async def on_message(message: cl.message):\n",
    "    chat_history = cl.user_session.get(\"chat_history\")\n",
    "\n",
    "    model = \"qwen2.5:0.5b\"   # Selected LLM model from Ollama\n",
    "\n",
    "    chat_history.append({'role':'user', 'content':message.content})\n",
    "\n",
    "    cb = cl.Message(content=\"\")\n",
    "    await cb.send()\n",
    "\n",
    "    def generate_chunks():\n",
    "        return ollama.chat(\n",
    "            model=model,\n",
    "            messages=chat_history,\n",
    "            stream=True,\n",
    "            options={'stop': ['<|im_end|>']},\n",
    "        )\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    stream = await loop.run_in_executor(None, generate_chunks)\n",
    "\n",
    "    assistant_response = ''\n",
    "    for chunk in stream:\n",
    "        content = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "        if content:\n",
    "            assistant_response += content\n",
    "            await cb.stream_token(content)\n",
    "\n",
    "    chat_history.append({'role':'assistant', 'content':assistant_response})\n",
    "\n",
    "    await cb.update()\n",
    "```\n",
    "\n",
    "# ✅ Notebook Completed\n",
    "\n",
    "This notebook replicates the original Chainlit + Ollama integration with proper explanations and modular code organization.\n",
    "\n",
    "You can run and experiment with each section individually.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
